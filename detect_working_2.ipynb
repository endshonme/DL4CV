{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "getting values...\n",
      "loading data...\n",
      "getting values...\n",
      "Train on 2404 samples, validate on 1185 samples\n",
      "Epoch 1/3\n",
      "1680/2404 [===================>..........] - ETA: 5:55 - loss: 1.9449 - acc: 0.1482"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "########## TRAINING DATA ##########\n",
    "\n",
    "print('loading data...')\n",
    "# load dataset\n",
    "#dataframe = pd.read_csv('data/train.xlsx', header=None, nrows=1000)\n",
    "# read them in\n",
    "excel = pd.ExcelFile('data/validate.xlsx')\n",
    "    \n",
    "# turn them into dataframes\n",
    "dataframe = excel.parse(excel.sheet_names[0], header=None,index_col=None)\n",
    "\n",
    "print('getting values...')\n",
    "dataset = dataframe.values\n",
    "Y = np.array(dataset[:,:-1].astype(float))\n",
    "X = np.array(dataset[:,-1])\n",
    "\n",
    "#one-hot encoding\n",
    "Y = Y.reshape(-1)\n",
    "s = pd.Series(Y)\n",
    "Y = np.array(pd.get_dummies(s))\n",
    "\n",
    "X = [list(map(int, X[i].split())) for i in range(len(X))]\n",
    "X = np.array(X)\n",
    "\n",
    "# multidim array\n",
    "X = np.array([np.split(x_i,48) for x_i in X])\n",
    "\n",
    "########## TEST DATA ##########\n",
    "\n",
    "print('loading data...')\n",
    "# load dataset\n",
    "#dataframe = pd.read_csv('data/test.xlsx', header=0)\n",
    "# read them in\n",
    "excel = pd.ExcelFile('data/test.xlsx')\n",
    "    \n",
    "# turn them into dataframes\n",
    "dataframe = excel.parse(excel.sheet_names[0], header=None,index_col=None)\n",
    "print('getting values...')\n",
    "dataset = dataframe.values\n",
    "Y_t = np.array(dataset[:,:-1].astype(float))\n",
    "X_t = np.array(dataset[:,-1])\n",
    "\n",
    "#one-hot encoding\n",
    "Y_t = Y_t.reshape(-1)\n",
    "s = pd.Series(Y_t)\n",
    "Y_t = np.array(pd.get_dummies(s))\n",
    "\n",
    "X_t = [list(map(int, X_t[i].split())) for i in range(len(X_t))]\n",
    "X_t = np.array(X_t)\n",
    "\n",
    "# multidim array\n",
    "X_t = np.array([np.split(x_i,48) for x_i in X_t])\n",
    "\n",
    "########## DATA PREPROCESSING ##########\n",
    "\n",
    "X_t = X_t.reshape(X_t.shape[0], 48, 48, 1)\n",
    "X = X.reshape(X.shape[0], 48, 48, 1)\n",
    "\n",
    "X = X.astype('float32')\n",
    "X_t = X_t.astype('float32')\n",
    "X /= 255\n",
    "X_t /= 255\n",
    "\n",
    "########## LOSS FUNCTION ###########\n",
    "\n",
    "def custom_objective(y_true, y_pred):\n",
    "    '''Just another crossentropy'''\n",
    "    y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    y_pred /= y_pred.sum(axis=-1, keepdims=True)\n",
    "    cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n",
    "    return cce\n",
    "\n",
    "# Add it to optimiser like this\n",
    "# model.compile(loss= custom_objective,\n",
    "#              optimizer=keras.optimizers.Adam(lr=1e-7),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "########## KERAS ML MODEL ##########\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu', input_shape=(48,48, 1), padding='same'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu', padding='valid'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu', padding='valid'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu', padding='valid'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu', padding='valid'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# print (model.output_shape)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=1e-7, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, Y, validation_split=0.33, batch_size=48, epochs=3, verbose=1)\n",
    "scores = model.evaluate(X_t, Y_t, verbose=0)\n",
    "\n",
    "print(model.summary())\n",
    "print(scores)\n",
    "\n",
    "# output model visualization to file\n",
    "plot_model(model, to_file='out/model.png')\n",
    "\n",
    "# ########## SAVE MODEL ##########\n",
    "\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"out/model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "########## LOAD MODEL ##########\n",
    "\n",
    "# json_file = open('dillion_model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"dillion_model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "#\n",
    "# # evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(X_t, Y_t, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "#\n",
    "# prediction = loaded_model.predict(X, batch_size=1, verbose=0)\n",
    "# print(\"prediction of emotion\", prediction)\n",
    "#\n",
    "# predict_1 = prediction[0]\n",
    "# emotion_arr = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "#\n",
    "# def emotion(arr):\n",
    "#     index = 0\n",
    "#     highest_num = 0\n",
    "#     for i in range(7):\n",
    "#         if (arr[i] > highest_num):\n",
    "#             index = i\n",
    "#             highest_num = arr[i]\n",
    "#     return emotion_arr[index]\n",
    "#\n",
    "# for p in prediction:\n",
    "#     print(emotion(p), p)\n",
    "\n",
    "########## SHOW GRAPH OF RESULTS ##########\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
